# -*- coding: utf-8 -*-
"""Copy of GPU_CNN_Inference.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1I-kq93xnmIdWWmKCfy5MbFxAp3hdcojP
"""

!pip install timm -q

import torch
import torchvision.models as tv
import timm
import pandas as pd
import matplotlib.pyplot as plt
import time

print("=" * 60)
print("GPU SETUP")
print("=" * 60)
device = "cuda" if torch.cuda.is_available() else "cpu"
if device == "cuda":
      print(f"✓ GPU: {torch.cuda.get_device_name(0)}")
      print(f"✓ CUDA Version: {torch.version.cuda}")
      print(f"✓ Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")
else:
      print(" No GPU found!")

torch.backends.cudnn.benchmark = True

"""### ============================================
### LOAD MODELS
### ============================================
"""

resnet=tv.resnet50(weights=tv.ResNet50_Weights.IMAGENET1K_V2).eval().to(device)
vit=timm.create_model("vit_base_patch16_224", pretrained=True).eval().to(device)
print("ResNet-50 loaded")
print("ViT-B/16 loaded")

def benchmark_model(model,model_name,batch_sizes=[1,8,16,32],use_fp16=False,use_jit=False):
  results=[]
  if use_jit:
    example=torch.randn(1,3,224,224).to(device)
    model=torch.jit.trace(model,example)
    model=torch.jit.optimize_for_inference(model)
  for batch_size in batch_sizes:
    input_tensor=torch.randn(batch_size,3,224,224).to(device)
    with torch.inference_mode():
      for _ in range(10):
        if use_fp16:
          with torch.autocast(device_type="cuda", dtype=torch.float16):
            _=model(input_tensor)
        else:
          _=model(input_tensor)
    torch.cuda.synchronize()
    times=[]
    for _ in range(100):
      torch.cuda.reset_peak_memory_stats()
      start_event=torch.cuda.Event(enable_timing=True)
      end_event=torch.cuda.Event(enable_timing=True)
      start_event.record()
      with torch.inference_mode():
        if use_fp16:
          with torch.autocast(device_type="cuda", dtype=torch.float16):
            _=model(input_tensor)
        else:
          _=model(input_tensor)
      end_event.record()
      torch.cuda.synchronize()
      times.append(start_event.elapsed_time(end_event))
  avg_latency=sum(times)/len(times)
  p50_latency=sorted(times)[len(times)//2]
  p90_latency=sorted(times)[int(len(times)*0.9)]
  throughput=(batch_size/avg_latency)*1000
  peak_memory=torch.cuda.max_memory_allocated()/(1024**2)
  config = f"{'FP16' if use_fp16 else 'FP32'} {'JIT' if use_jit else 'Eager'}"
  results.append({
      'Model':model_name,
      'Batch Size':batch_size,
      'Config':config,
      'Avg Latency (ms)':round(avg_latency,2),
      'P50 Latency (ms)':round(p50_latency,2),
      'P90 Latency (ms)':round(p90_latency,2),
      'Throughput (img/s)':round(throughput,1),
      'Peak Memory (MB)':round(peak_memory,1)
  })
  return results

all_results=[]
all_results+=benchmark_model(resnet,"ResNet-50",use_fp16=False,use_jit=False)
all_results+=benchmark_model(resnet,"ResNet-50",use_fp16=True,use_jit=False)
all_results+=benchmark_model(resnet,"ResNet-50",use_fp16=False,use_jit=True)
#all_results+=benchmark_model(resnet,"ResNet-50",use_fp16=True,use_jit=True)
all_results+=benchmark_model(vit,"ViT-B/16", use_fp16=False, use_jit=False)
all_results+=benchmark_model(vit,"ViT-B/16", use_fp16=True, use_jit=False)
df=pd.DataFrame(all_results)
df.to_csv("benchmark_results.csv",index=False)

fig, axes = plt.subplots(1, 2, figsize=(14, 5))

for model_name, ax in zip(['ResNet-50', 'ViT-B/16'], axes):
      model_df = df[df['Model'] == model_name]

      for config in model_df['Config'].unique():
          config_df = model_df[model_df['Config'] == config]
          ax.plot(config_df['Batch Size'], config_df['Avg Latency (ms)'],
                  marker='o', label=config)

      ax.set_xlabel('Batch Size')
      ax.set_ylabel('Average Latency (ms)')
      ax.set_title(f'{model_name} - Latency Comparison')
      ax.legend()
      ax.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('latency_comparison.png', dpi=150, bbox_inches='tight')
plt.show()

fig, axes = plt.subplots(1, 2, figsize=(14, 5))

for model_name, ax in zip(['ResNet-50', 'ViT-B/16'], axes):
      model_df = df[df['Model'] == model_name]

      for config in model_df['Config'].unique():
          config_df = model_df[model_df['Config'] == config]
          ax.plot(config_df['Batch Size'], config_df['Throughput (img/s)'],
                  marker='o', label=config)

      ax.set_xlabel('Batch Size')
      ax.set_ylabel('Throughput (images/second)')
      ax.set_title(f'{model_name} - Throughput Comparison')
      ax.legend()
      ax.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('throughput_comparison.png', dpi=150, bbox_inches='tight')
plt.show()

